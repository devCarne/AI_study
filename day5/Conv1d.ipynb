{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결측치 수: 0\n",
      "중복데이터 수: 24\n"
     ]
    },
    {
     "data": {
      "text/plain": "                 Date Time  p (mbar)  T (degC)  Tdew (degC)  rh (%)  \\\n78767  01.07.2010 00:20:00    992.02     17.82        14.03   78.50   \n78773  01.07.2010 01:20:00    991.85     16.90        13.74   81.60   \n78779  01.07.2010 02:20:00    991.53     16.23        13.58   84.30   \n78785  01.07.2010 03:20:00    991.20     15.39        13.24   87.00   \n78791  01.07.2010 04:20:00    991.09     14.87        12.94   88.20   \n78797  01.07.2010 05:20:00    991.16     16.00        13.15   83.20   \n78803  01.07.2010 06:20:00    991.05     18.24        13.53   74.00   \n78809  01.07.2010 07:20:00    991.32     19.46        13.98   70.60   \n78815  01.07.2010 08:20:00    991.03     21.48        14.29   63.60   \n78821  01.07.2010 09:20:00    990.74     23.85        15.09   58.00   \n78827  01.07.2010 10:20:00    990.58     25.58        15.02   52.07   \n78833  01.07.2010 11:20:00    990.47     27.17        12.67   40.70   \n78839  01.07.2010 12:20:00    990.24     27.70        12.96   40.21   \n78845  01.07.2010 13:20:00    989.96     27.55        13.32   41.52   \n78851  01.07.2010 14:20:00    989.90     27.98        12.69   38.88   \n78857  01.07.2010 15:20:00    989.69     28.02        12.98   39.53   \n78863  01.07.2010 16:20:00    989.36     27.86        13.20   40.46   \n78869  01.07.2010 17:20:00    989.11     26.78        13.34   43.50   \n78875  01.07.2010 18:20:00    989.10     26.71        13.70   44.73   \n78881  01.07.2010 19:20:00    989.22     25.62        14.49   50.20   \n78887  01.07.2010 20:20:00    989.43     23.77        14.66   56.70   \n78893  01.07.2010 21:20:00    989.93     22.60        14.62   60.68   \n78899  01.07.2010 22:20:00    990.22     21.91        14.61   63.22   \n78905  01.07.2010 23:20:00    990.45     20.79        14.75   68.34   \n\n       VPmax (mbar)  VPact (mbar)  VPdef (mbar)  sh (g/kg)  H2OC (mmol/mol)  \\\n78767         20.44         16.04          4.39      10.12            16.17   \n78773         19.28         15.74          3.55       9.93            15.87   \n78779         18.48         15.58          2.90       9.83            15.71   \n78785         17.51         15.24          2.28       9.62            15.37   \n78791         16.94         14.94          2.00       9.43            15.07   \n78797         18.21         15.15          3.06       9.56            15.29   \n78803         20.99         15.53          5.46       9.80            15.67   \n78809         22.65         15.99          6.66      10.09            16.13   \n78815         25.65         16.32          9.34      10.30            16.46   \n78821         29.62         17.18         12.44      10.86            17.34   \n78827         32.84         17.10         15.74      10.81            17.26   \n78833         36.07         14.68         21.39       9.27            14.82   \n78839         37.20         14.96         22.24       9.45            15.11   \n78845         36.88         15.31         21.56       9.68            15.47   \n78851         37.81         14.70         23.11       9.29            14.85   \n78857         37.90         14.98         22.92       9.47            15.14   \n78863         37.55         15.19         22.36       9.61            15.36   \n78869         35.25         15.33         19.92       9.70            15.50   \n78875         35.10         15.70         19.40       9.93            15.88   \n78881         32.92         16.52         16.39      10.46            16.70   \n78887         29.48         16.71         12.76      10.57            16.89   \n78893         27.47         16.67         10.80      10.54            16.84   \n78899         26.34         16.65          9.69      10.53            16.81   \n78905         24.59         16.80          7.79      10.62            16.97   \n\n       rho (g/m**3)  wv (m/s)  max. wv (m/s)  wd (deg)  \n78767       1180.38      0.23           0.48     52.64  \n78773       1184.06      1.21           1.76    239.50  \n78779       1186.49      0.44           1.14    227.90  \n78785       1189.70      1.35           1.88    201.40  \n78791       1191.85      0.46           0.78     82.30  \n78797       1187.18      0.77           1.08    163.50  \n78803       1177.75      0.58           1.04     86.90  \n78809       1172.96      1.51           2.96     44.23  \n78815       1164.43      1.93           3.08     50.90  \n78821       1154.41      0.93           2.00    205.90  \n78827       1147.58      1.82           2.88     68.12  \n78833       1142.43      1.65           3.44     23.77  \n78839       1140.03      1.94           4.40    311.10  \n78845       1140.12      1.70           3.32    325.70  \n78851       1138.69      1.60           4.60      8.58  \n78857       1138.18      1.64           3.04    344.80  \n78863       1138.31      1.28           2.04    345.60  \n78869       1142.05      2.54           3.88     51.16  \n78875       1142.15      2.30           3.04     28.16  \n78881       1146.09      1.90           3.04     17.77  \n78887       1153.39      2.89           4.20     30.98  \n78893       1158.57      1.81           2.24    346.10  \n78899       1161.62      0.65           1.12    344.90  \n78905       1166.25      0.22           0.60     70.20  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date Time</th>\n      <th>p (mbar)</th>\n      <th>T (degC)</th>\n      <th>Tdew (degC)</th>\n      <th>rh (%)</th>\n      <th>VPmax (mbar)</th>\n      <th>VPact (mbar)</th>\n      <th>VPdef (mbar)</th>\n      <th>sh (g/kg)</th>\n      <th>H2OC (mmol/mol)</th>\n      <th>rho (g/m**3)</th>\n      <th>wv (m/s)</th>\n      <th>max. wv (m/s)</th>\n      <th>wd (deg)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>78767</th>\n      <td>01.07.2010 00:20:00</td>\n      <td>992.02</td>\n      <td>17.82</td>\n      <td>14.03</td>\n      <td>78.50</td>\n      <td>20.44</td>\n      <td>16.04</td>\n      <td>4.39</td>\n      <td>10.12</td>\n      <td>16.17</td>\n      <td>1180.38</td>\n      <td>0.23</td>\n      <td>0.48</td>\n      <td>52.64</td>\n    </tr>\n    <tr>\n      <th>78773</th>\n      <td>01.07.2010 01:20:00</td>\n      <td>991.85</td>\n      <td>16.90</td>\n      <td>13.74</td>\n      <td>81.60</td>\n      <td>19.28</td>\n      <td>15.74</td>\n      <td>3.55</td>\n      <td>9.93</td>\n      <td>15.87</td>\n      <td>1184.06</td>\n      <td>1.21</td>\n      <td>1.76</td>\n      <td>239.50</td>\n    </tr>\n    <tr>\n      <th>78779</th>\n      <td>01.07.2010 02:20:00</td>\n      <td>991.53</td>\n      <td>16.23</td>\n      <td>13.58</td>\n      <td>84.30</td>\n      <td>18.48</td>\n      <td>15.58</td>\n      <td>2.90</td>\n      <td>9.83</td>\n      <td>15.71</td>\n      <td>1186.49</td>\n      <td>0.44</td>\n      <td>1.14</td>\n      <td>227.90</td>\n    </tr>\n    <tr>\n      <th>78785</th>\n      <td>01.07.2010 03:20:00</td>\n      <td>991.20</td>\n      <td>15.39</td>\n      <td>13.24</td>\n      <td>87.00</td>\n      <td>17.51</td>\n      <td>15.24</td>\n      <td>2.28</td>\n      <td>9.62</td>\n      <td>15.37</td>\n      <td>1189.70</td>\n      <td>1.35</td>\n      <td>1.88</td>\n      <td>201.40</td>\n    </tr>\n    <tr>\n      <th>78791</th>\n      <td>01.07.2010 04:20:00</td>\n      <td>991.09</td>\n      <td>14.87</td>\n      <td>12.94</td>\n      <td>88.20</td>\n      <td>16.94</td>\n      <td>14.94</td>\n      <td>2.00</td>\n      <td>9.43</td>\n      <td>15.07</td>\n      <td>1191.85</td>\n      <td>0.46</td>\n      <td>0.78</td>\n      <td>82.30</td>\n    </tr>\n    <tr>\n      <th>78797</th>\n      <td>01.07.2010 05:20:00</td>\n      <td>991.16</td>\n      <td>16.00</td>\n      <td>13.15</td>\n      <td>83.20</td>\n      <td>18.21</td>\n      <td>15.15</td>\n      <td>3.06</td>\n      <td>9.56</td>\n      <td>15.29</td>\n      <td>1187.18</td>\n      <td>0.77</td>\n      <td>1.08</td>\n      <td>163.50</td>\n    </tr>\n    <tr>\n      <th>78803</th>\n      <td>01.07.2010 06:20:00</td>\n      <td>991.05</td>\n      <td>18.24</td>\n      <td>13.53</td>\n      <td>74.00</td>\n      <td>20.99</td>\n      <td>15.53</td>\n      <td>5.46</td>\n      <td>9.80</td>\n      <td>15.67</td>\n      <td>1177.75</td>\n      <td>0.58</td>\n      <td>1.04</td>\n      <td>86.90</td>\n    </tr>\n    <tr>\n      <th>78809</th>\n      <td>01.07.2010 07:20:00</td>\n      <td>991.32</td>\n      <td>19.46</td>\n      <td>13.98</td>\n      <td>70.60</td>\n      <td>22.65</td>\n      <td>15.99</td>\n      <td>6.66</td>\n      <td>10.09</td>\n      <td>16.13</td>\n      <td>1172.96</td>\n      <td>1.51</td>\n      <td>2.96</td>\n      <td>44.23</td>\n    </tr>\n    <tr>\n      <th>78815</th>\n      <td>01.07.2010 08:20:00</td>\n      <td>991.03</td>\n      <td>21.48</td>\n      <td>14.29</td>\n      <td>63.60</td>\n      <td>25.65</td>\n      <td>16.32</td>\n      <td>9.34</td>\n      <td>10.30</td>\n      <td>16.46</td>\n      <td>1164.43</td>\n      <td>1.93</td>\n      <td>3.08</td>\n      <td>50.90</td>\n    </tr>\n    <tr>\n      <th>78821</th>\n      <td>01.07.2010 09:20:00</td>\n      <td>990.74</td>\n      <td>23.85</td>\n      <td>15.09</td>\n      <td>58.00</td>\n      <td>29.62</td>\n      <td>17.18</td>\n      <td>12.44</td>\n      <td>10.86</td>\n      <td>17.34</td>\n      <td>1154.41</td>\n      <td>0.93</td>\n      <td>2.00</td>\n      <td>205.90</td>\n    </tr>\n    <tr>\n      <th>78827</th>\n      <td>01.07.2010 10:20:00</td>\n      <td>990.58</td>\n      <td>25.58</td>\n      <td>15.02</td>\n      <td>52.07</td>\n      <td>32.84</td>\n      <td>17.10</td>\n      <td>15.74</td>\n      <td>10.81</td>\n      <td>17.26</td>\n      <td>1147.58</td>\n      <td>1.82</td>\n      <td>2.88</td>\n      <td>68.12</td>\n    </tr>\n    <tr>\n      <th>78833</th>\n      <td>01.07.2010 11:20:00</td>\n      <td>990.47</td>\n      <td>27.17</td>\n      <td>12.67</td>\n      <td>40.70</td>\n      <td>36.07</td>\n      <td>14.68</td>\n      <td>21.39</td>\n      <td>9.27</td>\n      <td>14.82</td>\n      <td>1142.43</td>\n      <td>1.65</td>\n      <td>3.44</td>\n      <td>23.77</td>\n    </tr>\n    <tr>\n      <th>78839</th>\n      <td>01.07.2010 12:20:00</td>\n      <td>990.24</td>\n      <td>27.70</td>\n      <td>12.96</td>\n      <td>40.21</td>\n      <td>37.20</td>\n      <td>14.96</td>\n      <td>22.24</td>\n      <td>9.45</td>\n      <td>15.11</td>\n      <td>1140.03</td>\n      <td>1.94</td>\n      <td>4.40</td>\n      <td>311.10</td>\n    </tr>\n    <tr>\n      <th>78845</th>\n      <td>01.07.2010 13:20:00</td>\n      <td>989.96</td>\n      <td>27.55</td>\n      <td>13.32</td>\n      <td>41.52</td>\n      <td>36.88</td>\n      <td>15.31</td>\n      <td>21.56</td>\n      <td>9.68</td>\n      <td>15.47</td>\n      <td>1140.12</td>\n      <td>1.70</td>\n      <td>3.32</td>\n      <td>325.70</td>\n    </tr>\n    <tr>\n      <th>78851</th>\n      <td>01.07.2010 14:20:00</td>\n      <td>989.90</td>\n      <td>27.98</td>\n      <td>12.69</td>\n      <td>38.88</td>\n      <td>37.81</td>\n      <td>14.70</td>\n      <td>23.11</td>\n      <td>9.29</td>\n      <td>14.85</td>\n      <td>1138.69</td>\n      <td>1.60</td>\n      <td>4.60</td>\n      <td>8.58</td>\n    </tr>\n    <tr>\n      <th>78857</th>\n      <td>01.07.2010 15:20:00</td>\n      <td>989.69</td>\n      <td>28.02</td>\n      <td>12.98</td>\n      <td>39.53</td>\n      <td>37.90</td>\n      <td>14.98</td>\n      <td>22.92</td>\n      <td>9.47</td>\n      <td>15.14</td>\n      <td>1138.18</td>\n      <td>1.64</td>\n      <td>3.04</td>\n      <td>344.80</td>\n    </tr>\n    <tr>\n      <th>78863</th>\n      <td>01.07.2010 16:20:00</td>\n      <td>989.36</td>\n      <td>27.86</td>\n      <td>13.20</td>\n      <td>40.46</td>\n      <td>37.55</td>\n      <td>15.19</td>\n      <td>22.36</td>\n      <td>9.61</td>\n      <td>15.36</td>\n      <td>1138.31</td>\n      <td>1.28</td>\n      <td>2.04</td>\n      <td>345.60</td>\n    </tr>\n    <tr>\n      <th>78869</th>\n      <td>01.07.2010 17:20:00</td>\n      <td>989.11</td>\n      <td>26.78</td>\n      <td>13.34</td>\n      <td>43.50</td>\n      <td>35.25</td>\n      <td>15.33</td>\n      <td>19.92</td>\n      <td>9.70</td>\n      <td>15.50</td>\n      <td>1142.05</td>\n      <td>2.54</td>\n      <td>3.88</td>\n      <td>51.16</td>\n    </tr>\n    <tr>\n      <th>78875</th>\n      <td>01.07.2010 18:20:00</td>\n      <td>989.10</td>\n      <td>26.71</td>\n      <td>13.70</td>\n      <td>44.73</td>\n      <td>35.10</td>\n      <td>15.70</td>\n      <td>19.40</td>\n      <td>9.93</td>\n      <td>15.88</td>\n      <td>1142.15</td>\n      <td>2.30</td>\n      <td>3.04</td>\n      <td>28.16</td>\n    </tr>\n    <tr>\n      <th>78881</th>\n      <td>01.07.2010 19:20:00</td>\n      <td>989.22</td>\n      <td>25.62</td>\n      <td>14.49</td>\n      <td>50.20</td>\n      <td>32.92</td>\n      <td>16.52</td>\n      <td>16.39</td>\n      <td>10.46</td>\n      <td>16.70</td>\n      <td>1146.09</td>\n      <td>1.90</td>\n      <td>3.04</td>\n      <td>17.77</td>\n    </tr>\n    <tr>\n      <th>78887</th>\n      <td>01.07.2010 20:20:00</td>\n      <td>989.43</td>\n      <td>23.77</td>\n      <td>14.66</td>\n      <td>56.70</td>\n      <td>29.48</td>\n      <td>16.71</td>\n      <td>12.76</td>\n      <td>10.57</td>\n      <td>16.89</td>\n      <td>1153.39</td>\n      <td>2.89</td>\n      <td>4.20</td>\n      <td>30.98</td>\n    </tr>\n    <tr>\n      <th>78893</th>\n      <td>01.07.2010 21:20:00</td>\n      <td>989.93</td>\n      <td>22.60</td>\n      <td>14.62</td>\n      <td>60.68</td>\n      <td>27.47</td>\n      <td>16.67</td>\n      <td>10.80</td>\n      <td>10.54</td>\n      <td>16.84</td>\n      <td>1158.57</td>\n      <td>1.81</td>\n      <td>2.24</td>\n      <td>346.10</td>\n    </tr>\n    <tr>\n      <th>78899</th>\n      <td>01.07.2010 22:20:00</td>\n      <td>990.22</td>\n      <td>21.91</td>\n      <td>14.61</td>\n      <td>63.22</td>\n      <td>26.34</td>\n      <td>16.65</td>\n      <td>9.69</td>\n      <td>10.53</td>\n      <td>16.81</td>\n      <td>1161.62</td>\n      <td>0.65</td>\n      <td>1.12</td>\n      <td>344.90</td>\n    </tr>\n    <tr>\n      <th>78905</th>\n      <td>01.07.2010 23:20:00</td>\n      <td>990.45</td>\n      <td>20.79</td>\n      <td>14.75</td>\n      <td>68.34</td>\n      <td>24.59</td>\n      <td>16.80</td>\n      <td>7.79</td>\n      <td>10.62</td>\n      <td>16.97</td>\n      <td>1166.25</td>\n      <td>0.22</td>\n      <td>0.60</td>\n      <td>70.20</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                 Date Time  p (mbar)  T (degC)  Tdew (degC)  rh (%)  \\\n78623  01.07.2010 00:20:00    992.02     17.82        14.03    78.5   \n78767  01.07.2010 00:20:00    992.02     17.82        14.03    78.5   \n\n       VPmax (mbar)  VPact (mbar)  VPdef (mbar)  sh (g/kg)  H2OC (mmol/mol)  \\\n78623         20.44         16.04          4.39      10.12            16.17   \n78767         20.44         16.04          4.39      10.12            16.17   \n\n       rho (g/m**3)  wv (m/s)  max. wv (m/s)  wd (deg)  \n78623       1180.38      0.23           0.48     52.64  \n78767       1180.38      0.23           0.48     52.64  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date Time</th>\n      <th>p (mbar)</th>\n      <th>T (degC)</th>\n      <th>Tdew (degC)</th>\n      <th>rh (%)</th>\n      <th>VPmax (mbar)</th>\n      <th>VPact (mbar)</th>\n      <th>VPdef (mbar)</th>\n      <th>sh (g/kg)</th>\n      <th>H2OC (mmol/mol)</th>\n      <th>rho (g/m**3)</th>\n      <th>wv (m/s)</th>\n      <th>max. wv (m/s)</th>\n      <th>wd (deg)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>78623</th>\n      <td>01.07.2010 00:20:00</td>\n      <td>992.02</td>\n      <td>17.82</td>\n      <td>14.03</td>\n      <td>78.5</td>\n      <td>20.44</td>\n      <td>16.04</td>\n      <td>4.39</td>\n      <td>10.12</td>\n      <td>16.17</td>\n      <td>1180.38</td>\n      <td>0.23</td>\n      <td>0.48</td>\n      <td>52.64</td>\n    </tr>\n    <tr>\n      <th>78767</th>\n      <td>01.07.2010 00:20:00</td>\n      <td>992.02</td>\n      <td>17.82</td>\n      <td>14.03</td>\n      <td>78.5</td>\n      <td>20.44</td>\n      <td>16.04</td>\n      <td>4.39</td>\n      <td>10.12</td>\n      <td>16.17</td>\n      <td>1180.38</td>\n      <td>0.23</td>\n      <td>0.48</td>\n      <td>52.64</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복데이터 수: 0\n",
      "-9999.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18896\\3966997854.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"wv (m/s)\"][df[\"wv (m/s)\"] == -9999.0] = 0\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_18896\\3966997854.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"max. wv (m/s)\"][df[\"max. wv (m/s)\"] == -9999.0] = 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "zip_path = tensorflow.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
    "    fname='jena_climate_2009_2016.csv.zip',\n",
    "    extract=True)\n",
    "csv_path, _ = os.path.splitext(zip_path)\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df[5::6]\n",
    "# 결측치확인\n",
    "df = df.drop(\"Tpot (K)\", axis=1)\n",
    "print(\"결측치 수:\", df.isnull().sum().sum())\n",
    "# 중복데이터 확인\n",
    "print(\"중복데이터 수:\", df.duplicated().sum())\n",
    "display(df[df.duplicated()])\n",
    "display(df[df[\"Date Time\"] == \"01.07.2010 00:20:00\"])\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"중복데이터 수:\", df.duplicated().sum())\n",
    "# 이상치 확인 후 수정\n",
    "print(df[\"wv (m/s)\"].min())\n",
    "df[\"wv (m/s)\"][df[\"wv (m/s)\"] == -9999.0] = 0\n",
    "df[\"max. wv (m/s)\"][df[\"max. wv (m/s)\"] == -9999.0] = 0\n",
    "df[\"Date Time\"] = pd.to_datetime(df[\"Date Time\"], format=\"%d.%m.%Y %H:%M:%S\")\n",
    "df.set_index(\"Date Time\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_data: 2922\n",
      "num_time_data: 2915\n",
      "num_test: 875\n",
      "num_train: 2040\n",
      "train_index: 2047\n"
     ]
    }
   ],
   "source": [
    "df = df.resample(\"D\").mean().ffill()\n",
    "\n",
    "df.isnull().sum().sum()\n",
    "target = df[\"T (degC)\"]\n",
    "input = df[['Tdew (degC)', 'rh (%)', 'VPmax (mbar)', 'rho (g/m**3)', \"T (degC)\"]]\n",
    "\n",
    "num_data = len(target)\n",
    "print(\"num_data:\", num_data)\n",
    "time_step = 7\n",
    "num_time_data = num_data - time_step\n",
    "print(\"num_time_data:\", num_time_data)\n",
    "test_size = 0.3\n",
    "num_test = int(np.ceil((num_time_data) * 0.3))\n",
    "print(\"num_test:\", num_test)\n",
    "num_train = num_time_data - num_test\n",
    "print(\"num_train:\", num_train)\n",
    "train_index = num_train + time_step\n",
    "print(\"train_index:\", train_index)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "(2922, 5)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#표준화\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(input[:train_index])\n",
    "input_ss = ss.transform(input)\n",
    "input_ss.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "((2040, 7, 5), (875, 7, 5), (2040,), (875,))"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=test_size, shuffle=False, random_state=7)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:From C:\\Users\\Administrator\\IdeaProjects\\DataAnalysis\\Chapter04\\AI_Study\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator\\IdeaProjects\\DataAnalysis\\Chapter04\\AI_Study\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator\\IdeaProjects\\DataAnalysis\\Chapter04\\AI_Study\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator\\IdeaProjects\\DataAnalysis\\Chapter04\\AI_Study\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/21 [===================>..........] - ETA: 0s - loss: 78.4367 - mae: 7.1828   \n",
      "Epoch 1: val_loss improved from inf to 26.11892, saving model to best_model.hdf5\n",
      "21/21 [==============================] - 1s 13ms/step - loss: 63.7336 - mae: 6.3161 - val_loss: 26.1189 - val_mae: 3.8525\n",
      "Epoch 2/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 28.9670 - mae: 4.0930\n",
      "Epoch 2: val_loss improved from 26.11892 to 12.96344, saving model to best_model.hdf5\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 19.9352 - mae: 3.5533 - val_loss: 12.9634 - val_mae: 2.8655\n",
      "Epoch 3/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 14.2895 - mae: 2.9855"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\IdeaProjects\\DataAnalysis\\Chapter04\\AI_Study\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: val_loss improved from 12.96344 to 9.56487, saving model to best_model.hdf5\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 14.2226 - mae: 2.9756 - val_loss: 9.5649 - val_mae: 2.4571\n",
      "Epoch 4/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 15.6310 - mae: 3.0107\n",
      "Epoch 4: val_loss improved from 9.56487 to 8.11913, saving model to best_model.hdf5\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 11.7426 - mae: 2.6323 - val_loss: 8.1191 - val_mae: 2.2820\n",
      "Epoch 5/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 8.0981 - mae: 2.2350\n",
      "Epoch 5: val_loss improved from 8.11913 to 6.86047, saving model to best_model.hdf5\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 10.4056 - mae: 2.4372 - val_loss: 6.8605 - val_mae: 2.0775\n",
      "Epoch 6/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 10.0889 - mae: 2.3318\n",
      "Epoch 6: val_loss improved from 6.86047 to 6.44477, saving model to best_model.hdf5\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 9.6357 - mae: 2.3118 - val_loss: 6.4448 - val_mae: 2.0202\n",
      "Epoch 7/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 8.9513 - mae: 2.2243\n",
      "Epoch 7: val_loss improved from 6.44477 to 6.02745, saving model to best_model.hdf5\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 9.2345 - mae: 2.2244 - val_loss: 6.0275 - val_mae: 1.9404\n",
      "Epoch 8/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 9.6699 - mae: 2.2589\n",
      "Epoch 8: val_loss improved from 6.02745 to 5.87117, saving model to best_model.hdf5\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 8.9782 - mae: 2.1868 - val_loss: 5.8712 - val_mae: 1.9110\n",
      "Epoch 9/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 8.8574 - mae: 2.1471\n",
      "Epoch 9: val_loss improved from 5.87117 to 5.82969, saving model to best_model.hdf5\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 8.7640 - mae: 2.1438 - val_loss: 5.8297 - val_mae: 1.8835\n",
      "Epoch 10/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 9.2951 - mae: 2.2409\n",
      "Epoch 10: val_loss improved from 5.82969 to 5.70800, saving model to best_model.hdf5\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.8168 - mae: 2.1445 - val_loss: 5.7080 - val_mae: 1.8653\n",
      "Epoch 11/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 8.2281 - mae: 2.0638\n",
      "Epoch 11: val_loss improved from 5.70800 to 5.58508, saving model to best_model.hdf5\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.5731 - mae: 2.0979 - val_loss: 5.5851 - val_mae: 1.8580\n",
      "Epoch 12/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 5.9735 - mae: 1.7657\n",
      "Epoch 12: val_loss did not improve from 5.58508\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.5022 - mae: 2.0895 - val_loss: 5.6006 - val_mae: 1.8606\n",
      "Epoch 13/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 7.7375 - mae: 2.1113\n",
      "Epoch 13: val_loss improved from 5.58508 to 5.47184, saving model to best_model.hdf5\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.4738 - mae: 2.0851 - val_loss: 5.4718 - val_mae: 1.8307\n",
      "Epoch 14/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 9.5264 - mae: 2.1191\n",
      "Epoch 14: val_loss did not improve from 5.47184\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.5569 - mae: 2.1008 - val_loss: 5.8168 - val_mae: 1.8848\n",
      "Epoch 15/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 9.4452 - mae: 2.2709\n",
      "Epoch 15: val_loss did not improve from 5.47184\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.3966 - mae: 2.0726 - val_loss: 5.4817 - val_mae: 1.8282\n",
      "Epoch 16/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 8.6448 - mae: 1.9640\n",
      "Epoch 16: val_loss did not improve from 5.47184\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.3162 - mae: 2.0595 - val_loss: 5.5296 - val_mae: 1.8489\n",
      "Epoch 17/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 8.4580 - mae: 2.0420\n",
      "Epoch 17: val_loss improved from 5.47184 to 5.45079, saving model to best_model.hdf5\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.3308 - mae: 2.0593 - val_loss: 5.4508 - val_mae: 1.8233\n",
      "Epoch 18/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 4.9191 - mae: 1.6626\n",
      "Epoch 18: val_loss did not improve from 5.45079\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.1731 - mae: 2.0246 - val_loss: 5.5739 - val_mae: 1.8330\n",
      "Epoch 19/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 6.5494 - mae: 1.9831\n",
      "Epoch 19: val_loss improved from 5.45079 to 5.33350, saving model to best_model.hdf5\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.2790 - mae: 2.0486 - val_loss: 5.3335 - val_mae: 1.8055\n",
      "Epoch 20/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 8.6888 - mae: 2.0155\n",
      "Epoch 20: val_loss did not improve from 5.33350\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.2303 - mae: 2.0476 - val_loss: 5.3573 - val_mae: 1.8080\n",
      "Epoch 21/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 10.6335 - mae: 2.1158\n",
      "Epoch 21: val_loss did not improve from 5.33350\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.1911 - mae: 2.0289 - val_loss: 5.3581 - val_mae: 1.8170\n",
      "Epoch 22/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 8.7218 - mae: 1.9736\n",
      "Epoch 22: val_loss did not improve from 5.33350\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.2847 - mae: 2.0531 - val_loss: 5.4867 - val_mae: 1.8440\n",
      "Epoch 23/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 9.6525 - mae: 2.2173\n",
      "Epoch 23: val_loss did not improve from 5.33350\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.1991 - mae: 2.0308 - val_loss: 5.4340 - val_mae: 1.8310\n",
      "Epoch 24/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 5.6709 - mae: 1.7819\n",
      "Epoch 24: val_loss did not improve from 5.33350\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.1613 - mae: 2.0287 - val_loss: 5.3933 - val_mae: 1.8169\n",
      "Epoch 25/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 5.8424 - mae: 1.9226\n",
      "Epoch 25: val_loss did not improve from 5.33350\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.1126 - mae: 2.0239 - val_loss: 5.5236 - val_mae: 1.8349\n",
      "Epoch 26/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 5.6410 - mae: 1.7484\n",
      "Epoch 26: val_loss did not improve from 5.33350\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.0956 - mae: 2.0108 - val_loss: 5.4337 - val_mae: 1.8249\n",
      "Epoch 27/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 12.5136 - mae: 2.4657\n",
      "Epoch 27: val_loss did not improve from 5.33350\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.1924 - mae: 2.0333 - val_loss: 5.8367 - val_mae: 1.9027\n",
      "Epoch 28/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 8.7792 - mae: 2.1221\n",
      "Epoch 28: val_loss did not improve from 5.33350\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.2576 - mae: 2.0501 - val_loss: 5.5845 - val_mae: 1.8643\n",
      "Epoch 29/100\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 8.0810 - mae: 2.0282\n",
      "Epoch 29: val_loss did not improve from 5.33350\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.1741 - mae: 2.0221 - val_loss: 5.9801 - val_mae: 1.9087\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dropout, Dense, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input((7, 5)))\n",
    "model.add(Conv1D(filters=128, kernel_size=4, activation=\"relu\"))\n",
    "#model.add(Conv1D(filters=64, kernel_size=4, activation=\"relu\"))\n",
    "#model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(1, activation=\"relu\"))\n",
    "model.compile(loss=\"mse\", optimizer=Adam(lr=0.0005), metrics=[\"mae\"])\n",
    "mc = ModelCheckpoint(filepath=\"best_model.hdf5\", save_best_only=True, verbose=1)\n",
    "es = EarlyStopping(patience=10)\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=100, epochs=100, callbacks=[es, mc])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 1ms/step - loss: 5.3335 - mae: 1.8055\n"
     ]
    },
    {
     "data": {
      "text/plain": "[5.33350133895874, 1.8054935932159424]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "best_model = load_model(\"best_model.hdf5\")\n",
    "best_model.evaluate(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s 954us/step - loss: 5.9801 - mae: 1.9087\n"
     ]
    },
    {
     "data": {
      "text/plain": "[5.980074882507324, 1.9087462425231934]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator\\IdeaProjects\\DataAnalysis\\Chapter04\\AI_Study\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Administrator\\IdeaProjects\\DataAnalysis\\Chapter04\\AI_Study\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 85.6807 - mae: 7.8615\n",
      "Epoch 1: val_loss improved from inf to 34.88361, saving model to best_model.hdf5\n",
      "17/17 [==============================] - 4s 54ms/step - loss: 82.9320 - mae: 7.7118 - val_loss: 34.8836 - val_mae: 4.8583\n",
      "Epoch 2/200\n",
      "11/17 [==================>...........] - ETA: 0s - loss: 22.8737 - mae: 3.8697"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\IdeaProjects\\DataAnalysis\\Chapter04\\AI_Study\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/17 [===========================>..] - ETA: 0s - loss: 20.9626 - mae: 3.6488\n",
      "Epoch 2: val_loss improved from 34.88361 to 14.36666, saving model to best_model.hdf5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 20.7703 - mae: 3.6308 - val_loss: 14.3667 - val_mae: 3.0520\n",
      "Epoch 3/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 14.3060 - mae: 2.9986\n",
      "Epoch 3: val_loss improved from 14.36666 to 11.91671, saving model to best_model.hdf5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 14.3672 - mae: 3.0069 - val_loss: 11.9167 - val_mae: 2.7953\n",
      "Epoch 4/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 11.7225 - mae: 2.7490\n",
      "Epoch 4: val_loss improved from 11.91671 to 8.84099, saving model to best_model.hdf5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 11.6821 - mae: 2.7434 - val_loss: 8.8410 - val_mae: 2.3927\n",
      "Epoch 5/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 9.8386 - mae: 2.5027\n",
      "Epoch 5: val_loss improved from 8.84099 to 7.73140, saving model to best_model.hdf5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 9.9477 - mae: 2.5174 - val_loss: 7.7314 - val_mae: 2.1776\n",
      "Epoch 6/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 8.2744 - mae: 2.3023\n",
      "Epoch 6: val_loss improved from 7.73140 to 6.01026, saving model to best_model.hdf5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 8.3404 - mae: 2.3055 - val_loss: 6.0103 - val_mae: 1.9521\n",
      "Epoch 7/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 7.0444 - mae: 2.1026\n",
      "Epoch 7: val_loss improved from 6.01026 to 5.29493, saving model to best_model.hdf5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 6.9861 - mae: 2.0917 - val_loss: 5.2949 - val_mae: 1.7787\n",
      "Epoch 8/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 6.2300 - mae: 1.9717\n",
      "Epoch 8: val_loss improved from 5.29493 to 5.14368, saving model to best_model.hdf5\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 6.2121 - mae: 1.9713 - val_loss: 5.1437 - val_mae: 1.7708\n",
      "Epoch 9/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 5.7018 - mae: 1.8740\n",
      "Epoch 9: val_loss improved from 5.14368 to 4.81352, saving model to best_model.hdf5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 5.7700 - mae: 1.8791 - val_loss: 4.8135 - val_mae: 1.6919\n",
      "Epoch 10/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 5.5946 - mae: 1.8435\n",
      "Epoch 10: val_loss did not improve from 4.81352\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 5.6126 - mae: 1.8499 - val_loss: 4.8292 - val_mae: 1.6815\n",
      "Epoch 11/200\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 5.4785 - mae: 1.8076\n",
      "Epoch 11: val_loss improved from 4.81352 to 4.79670, saving model to best_model.hdf5\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 5.4961 - mae: 1.8140 - val_loss: 4.7967 - val_mae: 1.7090\n",
      "Epoch 12/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 5.5444 - mae: 1.8326\n",
      "Epoch 12: val_loss did not improve from 4.79670\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 5.5620 - mae: 1.8370 - val_loss: 5.0866 - val_mae: 1.7192\n",
      "Epoch 13/200\n",
      "15/17 [=========================>....] - ETA: 0s - loss: 5.5367 - mae: 1.8303\n",
      "Epoch 13: val_loss did not improve from 4.79670\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 5.5564 - mae: 1.8347 - val_loss: 4.8402 - val_mae: 1.7176\n",
      "Epoch 14/200\n",
      "14/17 [=======================>......] - ETA: 0s - loss: 5.7857 - mae: 1.8734\n",
      "Epoch 14: val_loss did not improve from 4.79670\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 5.7170 - mae: 1.8689 - val_loss: 5.0705 - val_mae: 1.7843\n",
      "Epoch 15/200\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 5.5447 - mae: 1.8576\n",
      "Epoch 15: val_loss did not improve from 4.79670\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 5.5866 - mae: 1.8516 - val_loss: 5.0917 - val_mae: 1.7858\n",
      "Epoch 16/200\n",
      "13/17 [=====================>........] - ETA: 0s - loss: 5.4930 - mae: 1.8231\n",
      "Epoch 16: val_loss did not improve from 4.79670\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 5.4273 - mae: 1.8142 - val_loss: 5.0814 - val_mae: 1.7222\n",
      "Epoch 17/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 5.6710 - mae: 1.8546\n",
      "Epoch 17: val_loss did not improve from 4.79670\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 5.6710 - mae: 1.8546 - val_loss: 5.0552 - val_mae: 1.7493\n",
      "Epoch 18/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 5.3256 - mae: 1.7806\n",
      "Epoch 18: val_loss improved from 4.79670 to 4.74662, saving model to best_model.hdf5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 5.3662 - mae: 1.7926 - val_loss: 4.7466 - val_mae: 1.6928\n",
      "Epoch 19/200\n",
      "17/17 [==============================] - ETA: 0s - loss: 5.3159 - mae: 1.7822\n",
      "Epoch 19: val_loss improved from 4.74662 to 4.65748, saving model to best_model.hdf5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 5.3159 - mae: 1.7822 - val_loss: 4.6575 - val_mae: 1.6708\n",
      "Epoch 20/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 5.5452 - mae: 1.8317\n",
      "Epoch 20: val_loss did not improve from 4.65748\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 5.5411 - mae: 1.8319 - val_loss: 4.6582 - val_mae: 1.6541\n",
      "Epoch 21/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 5.3522 - mae: 1.8026\n",
      "Epoch 21: val_loss did not improve from 4.65748\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 5.3323 - mae: 1.7989 - val_loss: 5.3418 - val_mae: 1.8421\n",
      "Epoch 22/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 5.1827 - mae: 1.7736\n",
      "Epoch 22: val_loss did not improve from 4.65748\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 5.2348 - mae: 1.7861 - val_loss: 5.0784 - val_mae: 1.7781\n",
      "Epoch 23/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 5.2805 - mae: 1.7895\n",
      "Epoch 23: val_loss improved from 4.65748 to 4.63728, saving model to best_model.hdf5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 5.2808 - mae: 1.7851 - val_loss: 4.6373 - val_mae: 1.6659\n",
      "Epoch 24/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 5.1329 - mae: 1.7672\n",
      "Epoch 24: val_loss did not improve from 4.63728\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 5.1409 - mae: 1.7706 - val_loss: 5.0538 - val_mae: 1.7826\n",
      "Epoch 25/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 5.3286 - mae: 1.7969\n",
      "Epoch 25: val_loss did not improve from 4.63728\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 5.2962 - mae: 1.7919 - val_loss: 4.9968 - val_mae: 1.7730\n",
      "Epoch 26/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 5.2364 - mae: 1.7753\n",
      "Epoch 26: val_loss improved from 4.63728 to 4.60875, saving model to best_model.hdf5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 5.2665 - mae: 1.7832 - val_loss: 4.6088 - val_mae: 1.6555\n",
      "Epoch 27/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 5.2133 - mae: 1.7737\n",
      "Epoch 27: val_loss did not improve from 4.60875\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 5.2020 - mae: 1.7742 - val_loss: 5.1002 - val_mae: 1.7471\n",
      "Epoch 28/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 5.1801 - mae: 1.7725\n",
      "Epoch 28: val_loss did not improve from 4.60875\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 5.1832 - mae: 1.7751 - val_loss: 5.6339 - val_mae: 1.8893\n",
      "Epoch 29/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 5.1654 - mae: 1.7626\n",
      "Epoch 29: val_loss did not improve from 4.60875\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 5.1689 - mae: 1.7639 - val_loss: 4.9836 - val_mae: 1.7229\n",
      "Epoch 30/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 5.1247 - mae: 1.7626\n",
      "Epoch 30: val_loss did not improve from 4.60875\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 5.1399 - mae: 1.7656 - val_loss: 4.7217 - val_mae: 1.7099\n",
      "Epoch 31/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 5.0954 - mae: 1.7633\n",
      "Epoch 31: val_loss improved from 4.60875 to 4.59116, saving model to best_model.hdf5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 5.1220 - mae: 1.7684 - val_loss: 4.5912 - val_mae: 1.6479\n",
      "Epoch 32/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 5.0390 - mae: 1.7533\n",
      "Epoch 32: val_loss did not improve from 4.59116\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 5.0239 - mae: 1.7519 - val_loss: 4.7533 - val_mae: 1.6974\n",
      "Epoch 33/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 5.1457 - mae: 1.7619\n",
      "Epoch 33: val_loss did not improve from 4.59116\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 5.1036 - mae: 1.7527 - val_loss: 4.6936 - val_mae: 1.6637\n",
      "Epoch 34/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 4.8954 - mae: 1.7255\n",
      "Epoch 34: val_loss did not improve from 4.59116\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 4.9325 - mae: 1.7338 - val_loss: 4.6430 - val_mae: 1.6803\n",
      "Epoch 35/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 4.9148 - mae: 1.7185\n",
      "Epoch 35: val_loss did not improve from 4.59116\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 4.8993 - mae: 1.7149 - val_loss: 4.6351 - val_mae: 1.6673\n",
      "Epoch 36/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 4.8460 - mae: 1.7124\n",
      "Epoch 36: val_loss improved from 4.59116 to 4.54243, saving model to best_model.hdf5\n",
      "17/17 [==============================] - 0s 15ms/step - loss: 4.8261 - mae: 1.7088 - val_loss: 4.5424 - val_mae: 1.6409\n",
      "Epoch 37/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 4.8088 - mae: 1.7190\n",
      "Epoch 37: val_loss did not improve from 4.54243\n",
      "17/17 [==============================] - 0s 14ms/step - loss: 4.7984 - mae: 1.7167 - val_loss: 4.5618 - val_mae: 1.6382\n",
      "Epoch 38/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 4.8041 - mae: 1.7154\n",
      "Epoch 38: val_loss did not improve from 4.54243\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 4.8356 - mae: 1.7213 - val_loss: 4.8445 - val_mae: 1.6930\n",
      "Epoch 39/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 4.8183 - mae: 1.7177\n",
      "Epoch 39: val_loss did not improve from 4.54243\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 4.8581 - mae: 1.7264 - val_loss: 4.5843 - val_mae: 1.6474\n",
      "Epoch 40/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 4.8815 - mae: 1.7347\n",
      "Epoch 40: val_loss did not improve from 4.54243\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 4.8525 - mae: 1.7268 - val_loss: 4.5591 - val_mae: 1.6423\n",
      "Epoch 41/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 4.8692 - mae: 1.7229\n",
      "Epoch 41: val_loss did not improve from 4.54243\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 4.8958 - mae: 1.7259 - val_loss: 4.6901 - val_mae: 1.6814\n",
      "Epoch 42/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 4.8173 - mae: 1.7217\n",
      "Epoch 42: val_loss did not improve from 4.54243\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 4.8046 - mae: 1.7176 - val_loss: 4.5672 - val_mae: 1.6569\n",
      "Epoch 43/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 4.6891 - mae: 1.6866\n",
      "Epoch 43: val_loss did not improve from 4.54243\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 4.6901 - mae: 1.6871 - val_loss: 4.5730 - val_mae: 1.6478\n",
      "Epoch 44/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 4.6186 - mae: 1.6807\n",
      "Epoch 44: val_loss did not improve from 4.54243\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 4.6844 - mae: 1.6914 - val_loss: 4.5989 - val_mae: 1.6412\n",
      "Epoch 45/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 4.7402 - mae: 1.7061\n",
      "Epoch 45: val_loss did not improve from 4.54243\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 4.6988 - mae: 1.7000 - val_loss: 4.9168 - val_mae: 1.7395\n",
      "Epoch 46/200\n",
      "16/17 [===========================>..] - ETA: 0s - loss: 4.8698 - mae: 1.7342\n",
      "Epoch 46: val_loss did not improve from 4.54243\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 4.8389 - mae: 1.7289 - val_loss: 4.6212 - val_mae: 1.6627\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Dense, Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input((7, 5)))\n",
    "model.add(GRU(128, activation=\"tanh\", return_sequences=True))\n",
    "model.add(GRU(128, activation=\"tanh\"))\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\"])\n",
    "mc = ModelCheckpoint(filepath=\"best_model.hdf5\", save_best_only=True, verbose=1)\n",
    "es = EarlyStopping(patience=10)\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, batch_size=100, epochs=200, callbacks=[es, mc])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 1s 3ms/step - loss: 4.8456 - mae: 1.7116\n"
     ]
    },
    {
     "data": {
      "text/plain": "[4.845554828643799, 1.7115931510925293]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "best_model = load_model(\"best_model.hdf5\")\n",
    "best_model.evaluate(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
